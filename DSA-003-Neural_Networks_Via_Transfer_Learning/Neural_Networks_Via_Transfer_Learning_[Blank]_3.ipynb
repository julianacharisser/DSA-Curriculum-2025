{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EuWUsRa0l3o"
      },
      "outputs": [],
      "source": [
        "# === Cell 1 (Corrected): Install dependencies without version pinning ===\n",
        "!pip install \\\n",
        "    gradio \\\n",
        "    pandas \\\n",
        "    matplotlib \\\n",
        "    seaborn \\\n",
        "    scikit-learn \\\n",
        "    numpy \\\n",
        "    pillow \\\n",
        "    opencv-python \\\n",
        "    tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCZ_sFVW0ygv"
      },
      "outputs": [],
      "source": [
        "# === Cell 2 (Corrected): All imports ===\n",
        "\n",
        "# 1) Standard library\n",
        "import os, glob\n",
        "import io, base64\n",
        "\n",
        "# 2) Data & visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 3) TensorFlow & Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam, AdamW  # <-- FINAL CORRECTION HERE\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# 4) TensorFlow Add-Ons is no longer needed\n",
        "\n",
        "# 5) Scikit-learn utilities\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# 6) OpenCV & PIL (for any image I/O)\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import output, files\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hk6FER800T1"
      },
      "outputs": [],
      "source": [
        "# === Cell 3a: EDA – Emotion class distribution ===\n",
        "df = pd.read_csv('fer2013.csv')\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='emotion', data=df)\n",
        "plt.title(\"FER2013: Emotion Class Distribution\")\n",
        "plt.xlabel(\"Emotion (0–6)\"); plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn6Mud6hoiw8"
      },
      "outputs": [],
      "source": [
        "# === Cell 3b: EDA – Usage split ===\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.countplot(x='Usage', data=df,\n",
        "              order=['Training','PublicTest','PrivateTest'])\n",
        "plt.title(\"FER2013: Usage Split\"); plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UGm-necojP-"
      },
      "outputs": [],
      "source": [
        "# === Cell 3c: EDA – Pixel intensity histogram (sample) ===\n",
        "sample = np.fromstring(df.loc[0,'pixels'], dtype=int, sep=' ')\n",
        "plt.figure(figsize=(6,3))\n",
        "sns.histplot(sample, bins=50, kde=True)\n",
        "plt.title(\"Pixel Intensity Distribution\"); plt.xlabel(\"Value\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddH7Gi-iomIy"
      },
      "outputs": [],
      "source": [
        "# === Cell 3d: EDA – One random example per emotion ===\n",
        "fig, axes = plt.subplots(2,4, figsize=(12,6))\n",
        "fig.suptitle(\"Random Example per Emotion\", fontsize=16)\n",
        "for emo in range(7):\n",
        "    row = df[df.emotion==emo].sample(1).iloc[0]\n",
        "    img = np.fromstring(row.pixels, dtype=int, sep=' ').reshape(48,48)\n",
        "    ax = axes.flat[emo]\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f\"Label {emo}\")\n",
        "    ax.axis('off')\n",
        "axes.flat[-1].axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SJ9WbXOonmz"
      },
      "outputs": [],
      "source": [
        "# === Cell 4: Convert pixel strings → 48×48×3 uint8 arrays ===\n",
        "\n",
        "# 4.1: Drop any rows where ‘pixels’ doesn’t split into exactly 2304 values\n",
        "expected_len = 48*48\n",
        "mask = df['pixels'].apply(lambda s: isinstance(s, str) and len(s.split()) == expected_len)\n",
        "num_bad = len(df) - mask.sum()\n",
        "if num_bad:\n",
        "    print(f\"Dropping {num_bad} malformed rows (pixel‑count ≠ {expected_len})\")\n",
        "df = df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "# 4.2: Now safely convert each string to a (48,48,3) uint8 image\n",
        "def str_to_arr(s: str) -> np.ndarray:\n",
        "    pixels = s.split()                               # list of length 2304\n",
        "    arr48  = np.array(pixels, dtype=np.uint8)        # flat array\n",
        "    arr48  = arr48.reshape(48, 48)                   # to 2D\n",
        "    return np.stack((arr48,)*3, axis=-1)             # duplicate → 3 channels\n",
        "\n",
        "df['pixels_arr'] = df['pixels'].apply(str_to_arr)\n",
        "\n",
        "print(\"Success — all images are now (48,48,3):\", df['pixels_arr'].iloc[0].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kMzglHTopDg"
      },
      "outputs": [],
      "source": [
        "# === Cell 5: Split data, compute class_weights, and precompute per‑sample weights ===\n",
        "train_df = df[df['Usage']=='Training']\n",
        "val_df   = df[df['Usage']=='PublicTest']\n",
        "test_df  = df[df['Usage']=='PrivateTest']\n",
        "\n",
        "def extract(df_):\n",
        "    # X: (N,48,48,3) uint8 images; y: (N,) integer labels 0–6\n",
        "    X = np.stack(df_['pixels_arr'].values)\n",
        "    y = df_['emotion'].values.astype(int)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = extract(train_df)\n",
        "X_val,   y_val   = extract(val_df)\n",
        "X_test,  y_test  = extract(test_df)\n",
        "\n",
        "# compute balanced class weights {label: weight}\n",
        "classes = np.arange(7)\n",
        "weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(zip(classes, weights))\n",
        "\n",
        "# precompute a sample_weights array to pass into .flow(...)\n",
        "# sample_weights[i] = class_weights[y_train[i]]\n",
        "sample_weights = np.array([ class_weights[label] for label in y_train ])\n",
        "\n",
        "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"Class weights:\", class_weights)\n",
        "print(\"Sample weights shape:\", sample_weights.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsXM4ngUPJrJ"
      },
      "outputs": [],
      "source": [
        "# === Cell 6 (Corrected): build tf.data pipelines w/ reduced memory usage ===\n",
        "\n",
        "# 0) Constants\n",
        "num_classes  = 7\n",
        "batch_size   = 32\n",
        "img_height   = img_width = 128\n",
        "AUTOTUNE     = tf.data.AUTOTUNE\n",
        "n_train      = len(X_train)\n",
        "\n",
        "# 1) Base datasets from arrays\n",
        "base_train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "base_val_ds   = tf.data.Dataset.from_tensor_slices((X_val,   y_val))\n",
        "\n",
        "# 2) Class-wise subsets for oversampling\n",
        "class_ds = [\n",
        "    base_train_ds.filter(lambda x, y, cls=cls: tf.equal(y, cls))\n",
        "    for cls in range(num_classes)\n",
        "]\n",
        "\n",
        "# 3) Oversample minority classes\n",
        "train_ds = tf.data.Dataset.sample_from_datasets(\n",
        "    class_ds,\n",
        "    weights=[1 / num_classes] * num_classes\n",
        ")\n",
        "\n",
        "# 4) Cap to exactly n_train examples\n",
        "train_ds = train_ds.take(n_train)\n",
        "\n",
        "# 5) Strong in-graph augmentation\n",
        "aug = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomContrast(0.2),\n",
        "    layers.RandomTranslation(0.1,0.1)\n",
        "], name=\"strong_augmentation\")\n",
        "\n",
        "# 6) Preprocessing functions\n",
        "def preprocess_train(x, y):\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    x = tf.image.resize(x, [img_height, img_width])\n",
        "    x = aug(x, training=True)\n",
        "    x = preprocess_input(x)\n",
        "    y = tf.one_hot(tf.cast(y, tf.int32), depth=num_classes)\n",
        "    return x, y\n",
        "\n",
        "def preprocess_val(x, y):\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    x = tf.image.resize(x, [img_height, img_width])\n",
        "    x = preprocess_input(x)\n",
        "    y = tf.one_hot(tf.cast(y, tf.int32), depth=num_classes)\n",
        "    return x, y\n",
        "\n",
        "# 7) Final pipelines: map → shuffle/batch/prefetch\n",
        "train_ds = (\n",
        "    train_ds\n",
        "      .map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
        "      .shuffle(10000) # <-- FIX #2: Reduced shuffle buffer from 10,000 to 1000\n",
        "      .batch(batch_size)\n",
        "      .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    base_val_ds\n",
        "      .map(preprocess_val, num_parallel_calls=AUTOTUNE)\n",
        "      .batch(batch_size)\n",
        "      .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(f\"Prepared train_ds (oversampled to {n_train} examples) and val_ds at {img_height}×{img_width}, batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy0KmOms29Mi"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# === Cell 8: Left-Aligned Webcam & Upload UI with Continuous Emotion Prediction ===\n",
        "\n",
        "# --- Constants ---\n",
        "VIDEO_WIDTH, VIDEO_HEIGHT = 640, 480\n",
        "IMAGE_SIZE = 128 # model input height/width\n",
        "\n",
        "# --- Load model & build single-trace inference fn ---\n",
        "model = load_model(\"FER_MobileNetV2_best.h5\")\n",
        "@tf.function\n",
        "def predict_fn(x):\n",
        "    # run the model in inference mode, returns (batch, 7) tensor\n",
        "    return model(x, training=False)\n",
        "\n",
        "# --- Emotion labels & face detector ---\n",
        "emotion_labels = {\n",
        "    0: \"Angry\", 1: \"Disgust\", 2: \"Fear\",\n",
        "    3: \"Happy\", 4: \"Sad\",     5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "face_cascade = cv2.CascadeClassifier(\n",
        "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
        ")\n",
        "\n",
        "# --- Helper: detect faces, preprocess, predict, annotate ---\n",
        "def annotate_frame(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "    for (x, y, w, h) in faces:\n",
        "        face = frame[y:y+h, x:x+w]\n",
        "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "        resized = cv2.resize(face_rgb, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "        inp = preprocess_input(resized.astype(\"float32\"))\n",
        "        inp = tf.expand_dims(inp, axis=0)  # shape (1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "        preds = predict_fn(inp).numpy()[0]\n",
        "        idx = int(np.argmax(preds))\n",
        "        label = emotion_labels.get(idx, \"Unknown\")\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, label, (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    return frame\n",
        "\n",
        "# --- JavaScript UI: left-aligned video + bottom buttons ---\n",
        "def start_webcam_ui():\n",
        "    js = rf\"\"\"\n",
        "    <script>\n",
        "      const cid = 'emotion-container';\n",
        "      document.getElementById(cid)?.remove();\n",
        "      const c = document.createElement('div');\n",
        "      c.id = cid;\n",
        "      c.style.display = 'flex';\n",
        "      c.style.flexDirection = 'column';\n",
        "      c.style.alignItems = 'flex-start';\n",
        "      c.style.margin = '10px';\n",
        "      document.body.appendChild(c);\n",
        "\n",
        "      const v = document.createElement('video');\n",
        "      v.width = {VIDEO_WIDTH}; v.height = {VIDEO_HEIGHT}; v.autoplay = true;\n",
        "      c.appendChild(v);\n",
        "\n",
        "      navigator.mediaDevices.getUserMedia({{video:true}})\n",
        "        .then(s => {{\n",
        "          v.srcObject = s;\n",
        "          setTimeout(() => {{\n",
        "            const bar = document.createElement('div');\n",
        "            bar.style.marginTop = '10px';\n",
        "            c.appendChild(bar);\n",
        "\n",
        "            const snap = document.createElement('button');\n",
        "            snap.textContent = 'Capture Photo';\n",
        "            bar.appendChild(snap);\n",
        "\n",
        "            const upl = document.createElement('button');\n",
        "            upl.textContent = 'Upload Image';\n",
        "            bar.appendChild(upl);\n",
        "\n",
        "            snap.onclick = () => {{\n",
        "              const canvas = document.createElement('canvas');\n",
        "              canvas.width = {VIDEO_WIDTH};\n",
        "              canvas.height = {VIDEO_HEIGHT};\n",
        "              canvas.getContext('2d').drawImage(v, 0, 0);\n",
        "              const dataUrl = canvas.toDataURL('image/png');\n",
        "              google.colab.kernel.invokeFunction('notebook.on_capture', [dataUrl], {{}});\n",
        "            }};\n",
        "            upl.onclick = () => {{\n",
        "              google.colab.kernel.invokeFunction('notebook.on_upload', [], {{}});\n",
        "            }};\n",
        "          }}, 1000);\n",
        "        }})\n",
        "        .catch(e => alert('Webcam not accessible: ' + e));\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# --- Callback: process a captured screenshot ---\n",
        "def on_capture(dataUrl):\n",
        "    header, encoded = dataUrl.split(\",\", 1)\n",
        "    img = Image.open(io.BytesIO(base64.b64decode(encoded))).convert(\"RGB\")\n",
        "    frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    # frame is already VIDEO_WIDTH×VIDEO_HEIGHT\n",
        "    annotated = annotate_frame(frame)\n",
        "    cv2_imshow(annotated)\n",
        "\n",
        "# --- Callback: process an uploaded image ---\n",
        "def on_upload():\n",
        "    uploaded = files.upload()\n",
        "    for fname in uploaded:\n",
        "        img = Image.open(fname).convert(\"RGB\")\n",
        "        frame = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "        frame = cv2.resize(frame, (VIDEO_WIDTH, VIDEO_HEIGHT))\n",
        "        annotated = annotate_frame(frame)\n",
        "        cv2_imshow(annotated)\n",
        "\n",
        "# --- Register callbacks & launch UI ---\n",
        "output.register_callback('notebook.on_capture', on_capture)\n",
        "output.register_callback('notebook.on_upload', on_upload)\n",
        "start_webcam_ui()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}